{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_nytimes_search_results(company_name):\n",
    "    \"\"\"\n",
    "    Fetches search results from the New York Times website for a given company name.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to search for.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The HTTP response from the NYTimes search URL.\n",
    "    \"\"\"\n",
    "    # Define headers to mimic a real browser request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Construct the search URL\n",
    "    search_url = f\"https://www.nytimes.com/search?dropmab=false&lang=en&query={company_name}&sections=Business%7Cnyt%3A%2F%2Fsection%2F0415b2b0-513a-5e78-80da-21ab770cb753&sort=best&types=article\"\n",
    "\n",
    "    try:\n",
    "        # Make the HTTP GET request\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while fetching search results: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_info(url_response):\n",
    "    \"\"\"Extracts all the relevant information about the article from the URL response.\n",
    "    Returns a list of dictionaries containing \"title\", \"summary\", \"metadata\", etc.\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    soup = BeautifulSoup(url_response.text, \"html.parser\")\n",
    "\n",
    "    # Find all <a> tags that contain article information\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        try:\n",
    "            # Extract the link\n",
    "            link = a_tag[\"href\"]\n",
    "\n",
    "            # Extract the title\n",
    "            title_tag = a_tag.find(\"h4\", class_=\"css-nsjm9t\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "            # Only proceed if the title is available (assume it's an article)\n",
    "            if title:\n",
    "                # Extract summary\n",
    "                summary_tag = a_tag.find(\"p\", class_=\"css-e5tzus\")\n",
    "                summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "                # Extract source\n",
    "                source_tag = a_tag.find(\"span\", class_=\"css-chk81a\")\n",
    "                source = source_tag.get_text(strip=True) if source_tag else None\n",
    "\n",
    "                # Extract author\n",
    "                author_tag = a_tag.find(\"p\", class_=\"css-1engk30\")\n",
    "                author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "                # Extract and format timestamp\n",
    "                timestamp_span = a_tag.find(\"span\", class_=\"css-1t2tqhf\")\n",
    "                timestamp = None\n",
    "                if timestamp_span and timestamp_span.next_sibling:\n",
    "                    timestamp = timestamp_span.next_sibling.strip()\n",
    "                    if timestamp:\n",
    "                        timestamp = \", \".join(timestamp.split(\",\")[:2])  # Format timestamp\n",
    "\n",
    "                # Add article info to the list\n",
    "                articles.append({\n",
    "                    'link': link,\n",
    "                    'title': title,\n",
    "                    'source': source,\n",
    "                    'author': author,\n",
    "                    'timestamp': timestamp,\n",
    "                    'summary': summary\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing an article: {e}\")\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "url_response = fetch_nytimes_search_results(\"Tesla\")\n",
    "articles = extract_article_info(url_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to perform sentiment analysis on a list of articles using VADER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, articles):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentAnalyzer with a list of articles.\n",
    "\n",
    "        Args:\n",
    "            articles (list): A list of dictionaries containing article details.\n",
    "        \"\"\"\n",
    "        self.articles = articles\n",
    "        self.sia = SentimentIntensityAnalyzer()  # Initialize VADER sentiment analyzer\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyzes the sentiment of the given text using VADER.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            str: The sentiment label (\"positive\", \"negative\", or \"neutral\").\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"neutral\"  # Return neutral if text is empty\n",
    "\n",
    "        # Get sentiment scores\n",
    "        sentiment_scores = self.sia.polarity_scores(text)\n",
    "\n",
    "        # Determine sentiment based on compound score\n",
    "        if sentiment_scores[\"compound\"] >= 0.25:\n",
    "            return \"positive\"\n",
    "        elif sentiment_scores[\"compound\"] <= -0.25:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "\n",
    "    def analyze_articles(self):\n",
    "        \"\"\"\n",
    "        Performs sentiment analysis on all articles in the list.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries with added sentiment analysis results.\n",
    "        \"\"\"\n",
    "        for article in self.articles:\n",
    "            sentiment = self.analyze_sentiment(article.get(\"summary\"))\n",
    "            article[\"sentiment\"] = sentiment  # Add sentiment to the article dictionary\n",
    "\n",
    "        return self.articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and analyze articles\n",
    "sentiment_analyzer = SentimentAnalyzer(articles)\n",
    "results = sentiment_analyzer.analyze_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "class NewsTopicExtractor:\n",
    "    def __init__(self):\n",
    "        # Load spaCy English model\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except OSError:\n",
    "            print(\"Downloading spaCy English model...\")\n",
    "            spacy.cli.download(\"en_core_web_sm\")\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        # Stop words to filter out\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def extract_topics(self, summary, num_topics=3):\n",
    "        \"\"\"\n",
    "        Extract topics from a news summary\n",
    "        \n",
    "        Args:\n",
    "            summary (str): News summary text\n",
    "            num_topics (int): Number of topics to extract\n",
    "        \n",
    "        Returns:\n",
    "            list: Extracted topics\n",
    "        \"\"\"\n",
    "        # Process the summary with spaCy\n",
    "        doc = self.nlp(summary)\n",
    "        \n",
    "        # Extract named entities and nouns as potential topics\n",
    "        potential_topics = []\n",
    "        \n",
    "        # Add named entities\n",
    "        potential_topics.extend([ent.text for ent in doc.ents \n",
    "                                 if ent.label_ in ['ORG', 'PERSON', 'GPE', 'PRODUCT']])\n",
    "        \n",
    "        # Add important nouns and proper nouns\n",
    "        potential_topics.extend([token.text for token in doc \n",
    "                                 if token.pos_ in ['PROPN', 'NOUN'] \n",
    "                                 and token.text.lower() not in self.stop_words\n",
    "                                 and len(token.text) > 2])\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        topics = list(dict.fromkeys(potential_topics))\n",
    "        \n",
    "        # If not enough topics, use TF-IDF to extract more\n",
    "        if len(topics) < num_topics:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "            tfidf_matrix = vectorizer.fit_transform([summary])\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "            \n",
    "            # Get top TF-IDF terms\n",
    "            top_indices = tfidf_scores.argsort()[-num_topics:][::-1]\n",
    "            tfidf_topics = [feature_names[i] for i in top_indices]\n",
    "            \n",
    "            topics.extend(tfidf_topics)\n",
    "        \n",
    "        # Ensure unique topics and limit to num_topics\n",
    "        topics = list(dict.fromkeys(topics))[:num_topics]\n",
    "        \n",
    "        # Capitalize topics\n",
    "        topics = [topic.capitalize() for topic in topics]\n",
    "        \n",
    "        return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    try:\n",
    "        if isinstance(article, dict) and 'summary' in article:\n",
    "            extractor = NewsTopicExtractor()\n",
    "            topic_list = extractor.extract_topics(article[\"summary\"])\n",
    "            article[\"topics\"] = topic_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autopilot', 'Regulators', 'Automaker']\n",
      "['Twitter', 'Maker', 'Cars']\n",
      "['Head', 'Accounting', 'Weeks']\n",
      "['Disparity', 'Performance', 'Car']\n",
      "['California', 'Autopilot', 'Family']\n",
      "['Chicago', 'Temperatures', 'Batteries']\n",
      "['Florida', 'Tesla', 'Crash']\n",
      "['Elon musk', 'Tesla', 'Elon']\n",
      "['Tesla', 'Furor', 'Markets']\n",
      "['Trump', 'Teslas', 'President']\n"
     ]
    }
   ],
   "source": [
    "for article in articles:\n",
    "    print(article[\"topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_article_topics_pairs(articles):\n",
    "    \"\"\"\n",
    "    Analyzes topics across pairs of articles to find common and unique topics,\n",
    "    with case-insensitive comparison while preserving original case in results.\n",
    "    Returns a single list of common words across pairs.\n",
    "\n",
    "    Args:\n",
    "        articles: List of dictionaries, each containing a 'topics' key with a list of words\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with:\n",
    "        - 'common_words_across_pairs': List of words common to pairs of articles\n",
    "        - 'unique_words_in_article_X': List of words unique to each article (X is index, original case preserved)\n",
    "    \"\"\"\n",
    "    if not articles:\n",
    "        return {}\n",
    "\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        if 'topics' not in article or not isinstance(article['topics'], list):\n",
    "            processed_articles.append({'original': [], 'normalized': set(), 'case_mapping': {}})\n",
    "            continue\n",
    "\n",
    "        case_mapping = {}\n",
    "        normalized_topics = set()\n",
    "\n",
    "        for word in article['topics']:\n",
    "            normalized = word.lower()\n",
    "            if normalized not in case_mapping:\n",
    "                case_mapping[normalized] = word\n",
    "            normalized_topics.add(normalized)\n",
    "\n",
    "        processed_articles.append({\n",
    "            'original': article['topics'],\n",
    "            'normalized': normalized_topics,\n",
    "            'case_mapping': case_mapping\n",
    "        })\n",
    "\n",
    "    result = {\n",
    "        'common_words_across_pairs': []\n",
    "    }\n",
    "\n",
    "    # Find common words for each pair of articles and flatten into a single list\n",
    "    for i in range(0, len(processed_articles) - 1, 2):\n",
    "        article1 = processed_articles[i]\n",
    "        article2 = processed_articles[i + 1]\n",
    "\n",
    "        common_normalized = article1['normalized'].intersection(article2['normalized'])\n",
    "\n",
    "        if common_normalized:\n",
    "            common_words = [article1['case_mapping'].get(word, word) for word in common_normalized]\n",
    "            result['common_words_across_pairs'].extend(common_words)\n",
    "\n",
    "    # Find unique words for each article\n",
    "    for i, article in enumerate(processed_articles):\n",
    "        other_sets = [processed_articles[j]['normalized'] for j in range(len(processed_articles)) if j != i]\n",
    "\n",
    "        unique_normalized = article['normalized'] - set.union(*other_sets) if other_sets else article['normalized']\n",
    "\n",
    "        unique_words = []\n",
    "        if unique_normalized:\n",
    "            original_topics = article['original']\n",
    "            unique_words = [word for word in original_topics if word.lower() in unique_normalized]\n",
    "\n",
    "        result[f'unique_words_in_article_{i+1}'] = unique_words\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ans = analyze_article_topics_pairs(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'common_words_across_pairs': ['Tesla'], 'unique_words_in_article_1': ['Regulators', 'Automaker'], 'unique_words_in_article_2': ['Twitter', 'Maker', 'Cars'], 'unique_words_in_article_3': ['Head', 'Accounting', 'Weeks'], 'unique_words_in_article_4': ['Disparity', 'Performance', 'Car'], 'unique_words_in_article_5': ['California', 'Family'], 'unique_words_in_article_6': ['Chicago', 'Temperatures', 'Batteries'], 'unique_words_in_article_7': ['Florida', 'Crash'], 'unique_words_in_article_8': ['Elon musk', 'Elon'], 'unique_words_in_article_9': ['Furor', 'Markets'], 'unique_words_in_article_10': ['Trump', 'Teslas', 'President']}\n"
     ]
    }
   ],
   "source": [
    "print(top_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage comparision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "class CoverageComparison:\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"\n",
    "        Initialize the CoverageComparison class.\n",
    "        \n",
    "        Args:\n",
    "            api_key (str, optional): Gemini API key. If not provided, will try to load from environment.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def compare_two_articles(self, i, article1, article2, gemini_api_key):\n",
    "        \"\"\"\n",
    "        Generates a precise one-line comparison between two articles using Google's Gemini Flash model.\n",
    "        \n",
    "        Args:\n",
    "            article1 (dict): Dictionary with 'title' and 'summary' keys\n",
    "            article2 (dict): Dictionary with 'title' and 'summary' keys\n",
    "            gemini_api_key (str): Your Gemini API key\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\"Comparison\": \"one-line\", \"Impact\": \"one-line\"}\n",
    "        \"\"\"\n",
    "        client = genai.Client(api_key=self.api_key)\n",
    "        \n",
    "        prompt = f\"\"\"Compare these articles and respond in JSON format :\n",
    "\n",
    "        Article {i} - Title: {article1.get('title','')}\n",
    "        Summary: {article1.get('summary','')}\n",
    "\n",
    "        Article {i+1} - Title: {article2.get('title','')}\n",
    "        Summary: {article2.get('summary','')}\n",
    "\n",
    "        Provide:\n",
    "        1. \"Comparison\": One sentence highlighting key difference\n",
    "        2. \"Impact\": One sentence on practical consequence\n",
    "\n",
    "        Note: It should strictly avoid any other extra words like \"JSON\", etc.\n",
    "\n",
    "        Format exactly like this:\n",
    "        {{\n",
    "            \"Comparison\": \"Your one-line comparison here\",\n",
    "            \"Impact\": \"Your one-line impact here\"\n",
    "        }}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",  # Using the faster Flash model\n",
    "                contents=[prompt],\n",
    "                \n",
    "            )\n",
    "            \n",
    "            \n",
    "            # Extract the text and remove any markdown code block formatting\n",
    "            response_text = response.text.strip('`json\\n').strip('`').strip()\n",
    "\n",
    "            # Parse the JSON response\n",
    "            result = json.loads(response_text) \n",
    "\n",
    "            return {\n",
    "                \"Comparison\": \" \".join(result[\"Comparison\"].split()),\n",
    "                \"Impact\": \" \".join(result[\"Impact\"].split())\n",
    "            }\n",
    "        except json.JSONDecodeError as je:\n",
    "            print(f\"JSON Parsing Error: {je}\")\n",
    "            print(f\"Received response: {response.text}\")\n",
    "            return {\n",
    "                \"Comparison\": \"Parsing error\",\n",
    "                \"Impact\": \"Unable to parse response\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"API Error: {e}\")\n",
    "            return {\n",
    "                \"Comparison\": \"Comparison unavailable\",\n",
    "                \"Impact\": \"Impact analysis failed\"\n",
    "            }\n",
    "\n",
    "    def get_analysis_across_all(self, articles):\n",
    "        \"\"\"\n",
    "        Generates article comparisons across all the articles in pairs.\n",
    "        Args:\n",
    "            articles (list): List of dictionaries of articles with 'title' and 'summary' keys\n",
    "            gemini_api_key (str): Your Gemini API key\n",
    "\n",
    "        Returns:\n",
    "            list: List of dictionaries, each with \"Comparison\" and \"Impact\" keys.\n",
    "        \"\"\"\n",
    "        comparison_list = []\n",
    "        if len(articles) < 2:\n",
    "            return comparison_list # return empty list if less than 2 articles.\n",
    "\n",
    "        for i in range(0, len(articles) - 1, 2):\n",
    "            try:\n",
    "                ans_dict = self.compare_two_articles(i + 1, articles[i], articles[i + 1], self.api_key)\n",
    "                comparison_list.append(ans_dict)\n",
    "            except IndexError: # handle the case where there is an odd number of articles.\n",
    "                print(\"Odd number of articles, last one will be ignored\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "        return comparison_list\n",
    "    \n",
    "    def get_final_sentiment_analysis(self, comparisons):\n",
    "        \"\"\"\n",
    "        Generates a final sentiment analysis based on the impact of all article comparisons.\n",
    "        \n",
    "        Args:\n",
    "            comparisons (list): List of dictionaries, each with \"Comparison\" and \"Impact\" keys.\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\"Final Sentiment Analysis\": \"Two-line sentiment analysis\"}\n",
    "        \"\"\"\n",
    "        client = genai.Client(api_key=self.api_key)\n",
    "        \n",
    "        impacts = [comp[\"Impact\"] for comp in comparisons]\n",
    "        impacts_str = \"\\n\".join(impacts)\n",
    "\n",
    "        prompt = f\"\"\"Analyze the following impacts from news coverage and provide a final sentiment analysis in two lines:\n",
    "\n",
    "        {impacts_str}\n",
    "\n",
    "        Specifically address:\n",
    "        1. Whether the overall news coverage is positive or negative.\n",
    "        2. The overall impact on the company's market growth.\n",
    "\n",
    "        Respond in JSON format:\n",
    "        {{\n",
    "            \"Final Sentiment Analysis\": \"Your two-line analysis here\"\n",
    "        }}\n",
    "\n",
    "        Note: It should strictly avoid any other extra words like \"JSON\", etc.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=[prompt],\n",
    "            )\n",
    "            response_text = response.text.strip('`json\\n').strip('`').strip()\n",
    "            result = json.loads(response_text)\n",
    "            return {\"Final Sentiment Analysis\": \" \".join(result[\"Final Sentiment Analysis\"].split())}\n",
    "        except json.JSONDecodeError as je:\n",
    "            print(f\"JSON Parsing Error: {je}\")\n",
    "            print(f\"Received response: {response.text}\")\n",
    "            return {\"Final Sentiment Analysis\": \"Parsing error\"}\n",
    "        except Exception as e:\n",
    "            print(f\"API Error: {e}\")\n",
    "            return {\"Final Sentiment Analysis\": \"Analysis failed\"}\n",
    "        \n",
    "    def get_all_analysis(self, articles):\n",
    "        \"\"\"\n",
    "        Generates all comparison analysis and the final sentiment analysis.\n",
    "        \n",
    "        Args:\n",
    "            articles (list): List of dictionaries of articles with 'title' and 'summary' keys.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (comparison_list, final_sentiment)\n",
    "        \"\"\"\n",
    "        comparison_list = self.get_analysis_across_all(articles)\n",
    "        final_sentiment = self.get_final_sentiment_analysis(comparison_list)\n",
    "        return comparison_list, final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = CoverageComparison(gemini_api_key)\n",
    "c_list, final = compare.get_all_analysis(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Final Sentiment Analysis': 'The news coverage leans negative due to concerns about recalls, profitability, leadership, and increased scrutiny. This could hinder market growth by impacting investor confidence, consumer decisions, and overall brand perception.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep_translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in e:\\news-summarization\\venv\\lib\\site-packages (from deep_translator) (4.13.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in e:\\news-summarization\\venv\\lib\\site-packages (from deep_translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\news-summarization\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in e:\\news-summarization\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\news-summarization\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\news-summarization\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\news-summarization\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\news-summarization\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.3 kB ? eta -:--:--\n",
      "   -------------------------------------- - 41.0/42.3 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 42.3/42.3 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: deep_translator\n",
      "Successfully installed deep_translator-1.11.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from gtts import gTTS\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "class TextToSpeechConverter:\n",
    "    def __init__(self, output_directory=\"audio_outputs\"):\n",
    "        self.output_directory = output_directory\n",
    "        \n",
    "        # Remove existing directory and create a fresh one\n",
    "        if os.path.exists(self.output_directory):\n",
    "            shutil.rmtree(self.output_directory)\n",
    "        \n",
    "        # Create a new, empty directory\n",
    "        os.makedirs(self.output_directory)\n",
    "\n",
    "    def convert_english_to_hindi_audio(self, english_text, filename=None):\n",
    "        \"\"\"\n",
    "        Translates the given English text to Hindi, converts it to audio, and saves it as an MP3 file.\n",
    "\n",
    "        Args:\n",
    "            english_text (str): The English text to convert to audio.\n",
    "            filename (str, optional): The desired filename for the audio output. \n",
    "                                      If None, generates a default filename.\n",
    "        \n",
    "        Returns:\n",
    "            str or None: Path to the saved audio file, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate a default filename if not provided\n",
    "            if filename is None:\n",
    "                filename = f\"news_coverage.mp3\"\n",
    "            \n",
    "            # Translate English to Hindi using deep_translator\n",
    "            hindi_text = GoogleTranslator(source='auto', target='hi').translate(english_text)\n",
    "\n",
    "            # Convert Hindi text to audio\n",
    "            tts = gTTS(text=hindi_text, lang='hi')\n",
    "            filepath = os.path.join(self.output_directory, filename)\n",
    "            tts.save(filepath)\n",
    "            \n",
    "            print(f\"English text: {english_text}\")\n",
    "            print(f\"Hindi translation: {hindi_text}\")\n",
    "            print(f\"Audio saved to: {filepath}\")\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            print(f\"Error during audio conversion: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English text: The news coverage leans negative due to concerns about recalls, profitability, leadership, and increased scrutiny. This could hinder market growth by impacting investor confidence, consumer decisions, and overall brand perception.\n",
      "Hindi translation: समाचार कवरेज रिकॉल, लाभप्रदता, नेतृत्व और बढ़ी हुई जांच के बारे में चिंताओं के कारण नकारात्मक झुकता है। यह निवेशक के विश्वास, उपभोक्ता निर्णयों और समग्र ब्रांड धारणा को प्रभावित करके बाजार की वृद्धि में बाधा डाल सकता है।\n",
      "Audio saved to: audio_outputs\\news_coverage.mp3\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "converter = TextToSpeechConverter()\n",
    "audio_file_path = converter.convert_english_to_hindi_audio(final[\"Final Sentiment Analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
