{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_nytimes_search_results(company_name):\n",
    "    \"\"\"\n",
    "    Fetches search results from the New York Times website for a given company name.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to search for.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response: The HTTP response from the NYTimes search URL.\n",
    "    \"\"\"\n",
    "    # Define headers to mimic a real browser request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Construct the search URL\n",
    "    search_url = f\"https://www.nytimes.com/search?dropmab=false&lang=en&query={company_name}&sections=Business%7Cnyt%3A%2F%2Fsection%2F0415b2b0-513a-5e78-80da-21ab770cb753&sort=best&types=article\"\n",
    "\n",
    "    try:\n",
    "        # Make the HTTP GET request\n",
    "        response = requests.get(search_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while fetching search results: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_info(url_response):\n",
    "    \"\"\"Extracts all the relevant information about the article from the URL response.\n",
    "    Returns a list of dictionaries containing \"title\", \"summary\", \"metadata\", etc.\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    soup = BeautifulSoup(url_response.text, \"html.parser\")\n",
    "\n",
    "    # Find all <a> tags that contain article information\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        try:\n",
    "            # Extract the link\n",
    "            link = a_tag[\"href\"]\n",
    "\n",
    "            # Extract the title\n",
    "            title_tag = a_tag.find(\"h4\", class_=\"css-nsjm9t\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "            # Only proceed if the title is available (assume it's an article)\n",
    "            if title:\n",
    "                # Extract summary\n",
    "                summary_tag = a_tag.find(\"p\", class_=\"css-e5tzus\")\n",
    "                summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
    "\n",
    "                # Extract source\n",
    "                source_tag = a_tag.find(\"span\", class_=\"css-chk81a\")\n",
    "                source = source_tag.get_text(strip=True) if source_tag else None\n",
    "\n",
    "                # Extract author\n",
    "                author_tag = a_tag.find(\"p\", class_=\"css-1engk30\")\n",
    "                author = author_tag.get_text(strip=True) if author_tag else None\n",
    "\n",
    "                # Extract and format timestamp\n",
    "                timestamp_span = a_tag.find(\"span\", class_=\"css-1t2tqhf\")\n",
    "                timestamp = None\n",
    "                if timestamp_span and timestamp_span.next_sibling:\n",
    "                    timestamp = timestamp_span.next_sibling.strip()\n",
    "                    if timestamp:\n",
    "                        timestamp = \", \".join(timestamp.split(\",\")[:2])  # Format timestamp\n",
    "\n",
    "                # Add article info to the list\n",
    "                articles.append({\n",
    "                    'link': link,\n",
    "                    'title': title,\n",
    "                    'source': source,\n",
    "                    'author': author,\n",
    "                    'timestamp': timestamp,\n",
    "                    'summary': summary\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing an article: {e}\")\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "url_response = fetch_nytimes_search_results(\"Tesla\")\n",
    "articles = extract_article_info(url_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to perform sentiment analysis on a list of articles using VADER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, articles):\n",
    "        \"\"\"\n",
    "        Initializes the SentimentAnalyzer with a list of articles.\n",
    "\n",
    "        Args:\n",
    "            articles (list): A list of dictionaries containing article details.\n",
    "        \"\"\"\n",
    "        self.articles = articles\n",
    "        self.sia = SentimentIntensityAnalyzer()  # Initialize VADER sentiment analyzer\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyzes the sentiment of the given text using VADER.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to analyze.\n",
    "\n",
    "        Returns:\n",
    "            str: The sentiment label (\"positive\", \"negative\", or \"neutral\").\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"neutral\"  # Return neutral if text is empty\n",
    "\n",
    "        # Get sentiment scores\n",
    "        sentiment_scores = self.sia.polarity_scores(text)\n",
    "\n",
    "        # Determine sentiment based on compound score\n",
    "        if sentiment_scores[\"compound\"] >= 0.25:\n",
    "            return \"positive\"\n",
    "        elif sentiment_scores[\"compound\"] <= -0.25:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "\n",
    "    def analyze_articles(self):\n",
    "        \"\"\"\n",
    "        Performs sentiment analysis on all articles in the list.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries with added sentiment analysis results.\n",
    "        \"\"\"\n",
    "        for article in self.articles:\n",
    "            sentiment = self.analyze_sentiment(article.get(\"summary\"))\n",
    "            article[\"sentiment\"] = sentiment  # Add sentiment to the article dictionary\n",
    "\n",
    "        return self.articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and analyze articles\n",
    "sentiment_analyzer = SentimentAnalyzer(articles)\n",
    "results = sentiment_analyzer.analyze_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "class NewsTopicExtractor:\n",
    "    def __init__(self):\n",
    "        # Load spaCy English model\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except OSError:\n",
    "            print(\"Downloading spaCy English model...\")\n",
    "            spacy.cli.download(\"en_core_web_sm\")\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        # Stop words to filter out\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def extract_topics(self, summary, num_topics=3):\n",
    "        \"\"\"\n",
    "        Extract topics from a news summary\n",
    "        \n",
    "        Args:\n",
    "            summary (str): News summary text\n",
    "            num_topics (int): Number of topics to extract\n",
    "        \n",
    "        Returns:\n",
    "            list: Extracted topics\n",
    "        \"\"\"\n",
    "        # Process the summary with spaCy\n",
    "        doc = self.nlp(summary)\n",
    "        \n",
    "        # Extract named entities and nouns as potential topics\n",
    "        potential_topics = []\n",
    "        \n",
    "        # Add named entities\n",
    "        potential_topics.extend([ent.text for ent in doc.ents \n",
    "                                 if ent.label_ in ['ORG', 'PERSON', 'GPE', 'PRODUCT']])\n",
    "        \n",
    "        # Add important nouns and proper nouns\n",
    "        potential_topics.extend([token.text for token in doc \n",
    "                                 if token.pos_ in ['PROPN', 'NOUN'] \n",
    "                                 and token.text.lower() not in self.stop_words\n",
    "                                 and len(token.text) > 2])\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        topics = list(dict.fromkeys(potential_topics))\n",
    "        \n",
    "        # If not enough topics, use TF-IDF to extract more\n",
    "        if len(topics) < num_topics:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "            tfidf_matrix = vectorizer.fit_transform([summary])\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "            \n",
    "            # Get top TF-IDF terms\n",
    "            top_indices = tfidf_scores.argsort()[-num_topics:][::-1]\n",
    "            tfidf_topics = [feature_names[i] for i in top_indices]\n",
    "            \n",
    "            topics.extend(tfidf_topics)\n",
    "        \n",
    "        # Ensure unique topics and limit to num_topics\n",
    "        topics = list(dict.fromkeys(topics))[:num_topics]\n",
    "        \n",
    "        # Capitalize topics\n",
    "        topics = [topic.capitalize() for topic in topics]\n",
    "        \n",
    "        return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    try:\n",
    "        if isinstance(article, dict) and 'summary' in article:\n",
    "            extractor = NewsTopicExtractor()\n",
    "            topic_list = extractor.extract_topics(article[\"summary\"])\n",
    "            article[\"topics\"] = topic_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autopilot', 'Regulators', 'Automaker']\n",
      "['Twitter', 'Maker', 'Cars']\n",
      "['Head', 'Accounting', 'Weeks']\n",
      "['Disparity', 'Performance', 'Car']\n",
      "['California', 'Autopilot', 'Family']\n",
      "['Chicago', 'Temperatures', 'Batteries']\n",
      "['Florida', 'Tesla', 'Crash']\n",
      "['Elon musk', 'Tesla', 'Elon']\n",
      "['Tesla', 'Furor', 'Markets']\n",
      "['Trump', 'Teslas', 'President']\n"
     ]
    }
   ],
   "source": [
    "for article in articles:\n",
    "    print(article[\"topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
